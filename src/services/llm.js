/**
 * LLM Service - Integra√ß√£o com Google Gemini
 * Especializado em vender o Vendedor Digital Inteligente‚Ñ¢
 */

import { GoogleGenerativeAI } from '@google/generative-ai';
import { PRODUCT, METHOD, OBJECTIONS, POSITIONING } from '../knowledge/product.js';

class LLMService {
    constructor() {
        this.genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
        this.model = this.genAI.getGenerativeModel({
            model: process.env.LLM_MODEL || 'gemini-2.0-flash-exp'
        });
        this.fastModel = this.genAI.getGenerativeModel({
            model: 'gemini-2.0-flash-exp'
        });

        console.log(`ü§ñ LLM Service initialized with Gemini: ${process.env.LLM_MODEL || 'gemini-2.0-flash-exp'}`);
    }

    /**
     * Gera resposta conversacional
     */
    async generateResponse(systemPrompt, history, userMessage) {
        try {
            // Constr√≥i o contexto - Gemini exige que comece com 'user'
            let chatHistory = history.map(msg => ({
                role: msg.role === 'assistant' ? 'model' : 'user',
                parts: [{ text: msg.content }]
            }));

            // CORRE√á√ÉO: Garante que hist√≥rico comece com 'user'
            // Remove mensagens do in√≠cio at√© encontrar uma do 'user'
            while (chatHistory.length > 0 && chatHistory[0].role === 'model') {
                chatHistory.shift();
            }

            // CORRE√á√ÉO: Remove mensagens consecutivas do mesmo role (Gemini n√£o permite)
            chatHistory = chatHistory.filter((msg, index, arr) => {
                if (index === 0) return true;
                return msg.role !== arr[index - 1].role;
            });

            const chat = this.model.startChat({
                history: chatHistory,
                generationConfig: {
                    temperature: 0.75,
                    maxOutputTokens: 500,
                    topP: 0.9,
                    topK: 40
                }
            });

            // Adiciona system prompt ao contexto
            const fullMessage = `${systemPrompt}\n\n---\nMensagem do lead: ${userMessage}`;

            const result = await chat.sendMessage(fullMessage);
            const response = result.response.text();

            return response.trim();
        } catch (error) {
            console.error('‚ùå LLM Error:', error.message);
            throw error;
        }
    }

    /**
     * Analisa mensagem do usu√°rio
     */
    async analyzeMessage(message, context = '') {
        const prompt = `Analise esta mensagem de um lead interessado no ${PRODUCT.name}.

CONTEXTO DA CONVERSA:
${context}

MENSAGEM DO LEAD:
"${message}"

Responda APENAS com JSON v√°lido (sem markdown):
{
    "intent": "greeting|question|objection|buying_signal|farewell|frustration|interest|confirmation|rejection|other",
    "sentiment": "positive|neutral|negative",
    "buyingSignal": true ou false,
    "objection": "string ou null",
    "pain": "string ou null - dor de vendas/leads identificada",
    "urgency": "high|medium|low",
    "engagementLevel": "hot|warm|cold",
    "shouldSendAudio": true ou false (se a resposta ficaria melhor em √°udio)
}`;

        try {
            const result = await this.fastModel.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                generationConfig: { temperature: 0.1, maxOutputTokens: 300 }
            });

            const text = result.response.text();
            const cleaned = text.replace(/```json\n?|\n?```/g, '').trim();
            return JSON.parse(cleaned);
        } catch (error) {
            console.error('‚ùå Analysis Error:', error.message);
            return {
                intent: 'other',
                sentiment: 'neutral',
                buyingSignal: false,
                objection: null,
                pain: null,
                urgency: 'medium',
                engagementLevel: 'warm',
                shouldSendAudio: false
            };
        }
    }

    /**
     * Detecta motivo de abandono
     */
    async detectAbandonReason(history, conversationData) {
        const lastMessages = history.slice(-6).map(m => `${m.role}: ${m.content}`).join('\n');

        const prompt = `Analise por que este lead do ${PRODUCT.name} parou de responder.

√öLTIMAS MENSAGENS:
${lastMessages}

DADOS DO LEAD:
- Follow-ups j√° enviados: ${conversationData.follow_up_count}
- N√≠vel de engajamento: ${conversationData.engagement_level}
- Score: ${conversationData.qualification_score}
- Obje√ß√µes: ${conversationData.extracted_objections || 'nenhuma'}
- Dor: ${conversationData.extracted_pain || 'n√£o identificada'}

Responda APENAS com JSON v√°lido:
{
    "reason": "busy|objection|lost_interest|price|timing|forgot|competitor|not_qualified",
    "confidence": 0.0 a 1.0,
    "explanation": "breve explica√ß√£o",
    "suggestedApproach": "curiosity|reciprocity|fomo|scarcity|value|social_proof|empathy",
    "suggestedTone": "casual|professional|urgent|empathetic",
    "shouldCallInstead": true ou false
}`;

        try {
            const result = await this.fastModel.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                generationConfig: { temperature: 0.2, maxOutputTokens: 300 }
            });

            const text = result.response.text();
            const cleaned = text.replace(/```json\n?|\n?```/g, '').trim();
            return JSON.parse(cleaned);
        } catch (error) {
            console.error('‚ùå Abandon Detection Error:', error.message);
            return {
                reason: 'busy',
                confidence: 0.5,
                explanation: 'N√£o foi poss√≠vel determinar',
                suggestedApproach: 'value',
                suggestedTone: 'casual',
                shouldCallInstead: false
            };
        }
    }

    /**
     * Gera mensagem de follow-up personalizada
     */
    async generateFollowUp(context, strategy, followUpNumber) {
        const strategies = {
            curiosity: `Despertar curiosidade sobre o ${PRODUCT.name}.
Ex: "ei, lembrei de vc quando vi um lead morrendo no whats de um cliente..."`,
            reciprocity: `Oferecer valor primeiro.
Ex: "achei esse dado que pode te ajudar: 80% das vendas precisam de 5+ follow-ups..."`,
            fomo: `Fear of missing out.
Ex: "a galera que implementou t√° vendo os leads que iam morrer voltando..."`,
            scarcity: `Escassez e urg√™ncia real.
Ex: "to com a agenda quase lotada esse m√™s..."`,
            value: `Refor√ßar o valor e benef√≠cio.
Ex: "uma venda perdida por m√™s j√° paga o investimento..."`,
            social_proof: `Prova social.
Ex: "ontem fechei com mais um do mesmo nicho que o seu..."`,
            empathy: `Demonstrar compreens√£o.
Ex: "sei que deve estar corrido, sem problemas..."`
        };

        const escalation = {
            1: 'Leve e casual. S√≥ verificando.',
            2: 'Adicione valor. Mostre um dado √∫til.',
            3: 'Introduza FOMO suave.',
            4: 'Mais urgente. Use escassez real.',
            5: 'Despedida elegante. √öltimo contato.'
        };

        const prompt = `Gere uma mensagem de follow-up para vender o ${PRODUCT.name}.

POSICIONAMENTO OBRIGAT√ìRIO:
${POSITIONING.neverSay.map(s => `- NUNCA diga: "${s}"`).join('\n')}
${POSITIONING.alwaysSay.map(s => `- SEMPRE use: "${s}"`).join('\n')}

ESTRAT√âGIA: ${strategy}
${strategies[strategy]}

N√çVEL (${followUpNumber}/5): ${escalation[followUpNumber]}

CONTEXTO DO LEAD:
- Nome: ${context.name || 'n√£o identificado'}
- Dor: ${context.pain || 'leads morrendo no WhatsApp'}

REGRAS:
1. M√ÅXIMO 3 linhas curtas
2. Min√∫sculas (casual brasileiro)
3. M√°ximo 1 emoji
4. Termine for√ßando resposta
5. Use: vc, pq, tbm, t√°, mt

Responda APENAS com a mensagem:`;

        try {
            const result = await this.model.generateContent({
                contents: [{ role: 'user', parts: [{ text: prompt }] }],
                generationConfig: { temperature: 0.8, maxOutputTokens: 150 }
            });

            return result.response.text().trim();
        } catch (error) {
            console.error('‚ùå Follow-up Generation Error:', error.message);
            const fallbacks = {
                1: `e ai, sumiu? kk\nto aqui se quiser entender melhor\nfaz sentido conversar?`,
                2: `ei, sabia que 80% das vendas precisam de 5+ follow-ups?\na maioria desiste no primeiro`,
                3: `olha, cada dia que passa\nmais conversas morrem no whats`,
                4: `√∫ltima vez: consigo encaixar mais um projeto essa semana\numa venda perdida por m√™s j√° paga o investimento`,
                5: `beleza, vou parar de encher üòÖ\nse mudar de ideia √© s√≥ chamar`
            };
            return fallbacks[followUpNumber] || fallbacks[1];
        }
    }

    /**
     * Transcreve √°udio usando Gemini
     */
    async transcribeAudio(audioBuffer, mimeType = 'audio/ogg') {
        try {
            const base64Audio = audioBuffer.toString('base64');

            const result = await this.model.generateContent({
                contents: [{
                    role: 'user',
                    parts: [
                        { inlineData: { mimeType, data: base64Audio } },
                        { text: 'Transcreva este √°udio em portugu√™s brasileiro. Responda APENAS com a transcri√ß√£o, sem coment√°rios.' }
                    ]
                }],
                generationConfig: { temperature: 0.1, maxOutputTokens: 1000 }
            });

            return result.response.text().trim();
        } catch (error) {
            console.error('‚ùå Transcription Error:', error.message);
            return null;
        }
    }

    /**
     * Analisa imagem
     */
    async analyzeImage(imageBuffer, mimeType = 'image/jpeg', context = '') {
        try {
            const base64Image = imageBuffer.toString('base64');

            const prompt = `Analise esta imagem no contexto de uma conversa de vendas.
${context ? `Contexto: ${context}` : ''}

Descreva brevemente o que v√™ e se h√° algo relevante para a conversa de vendas.
Responda em portugu√™s brasileiro, de forma casual e curta.`;

            const result = await this.model.generateContent({
                contents: [{
                    role: 'user',
                    parts: [
                        { inlineData: { mimeType, data: base64Image } },
                        { text: prompt }
                    ]
                }],
                generationConfig: { temperature: 0.3, maxOutputTokens: 200 }
            });

            return result.response.text().trim();
        } catch (error) {
            console.error('‚ùå Image Analysis Error:', error.message);
            return null;
        }
    }

    /**
     * Decide se deve responder com √°udio
     * DESABILITADO - TTS n√£o implementado
     */
    shouldRespondWithAudio(analysis, conversationData) {
        // TTS n√£o implementado - sempre retorna false
        // TODO: Reativar quando implementar TTS
        return false;
    }

    /**
     * Gera √°udio a partir de texto (TTS)
     * NOTA: Por enquanto retorna null. Implementar quando adicionar Google Cloud TTS
     * ou outro servi√ßo de TTS.
     *
     * @param {string} text - Texto para converter em √°udio
     * @returns {Promise<Buffer|null>} Buffer do √°udio ou null se n√£o dispon√≠vel
     */
    async generateAudio(text) {
        // TODO: Implementar TTS quando adicionar biblioteca
        // Op√ß√µes:
        // 1. Google Cloud Text-to-Speech (@google-cloud/text-to-speech)
        // 2. ElevenLabs (elevenlabs-node)
        // 3. OpenAI TTS (openai)

        console.warn('‚ö†Ô∏è TTS n√£o implementado ainda. Use AUDIO_ENABLED=false no .env');
        return null;
    }
}

export default new LLMService();
